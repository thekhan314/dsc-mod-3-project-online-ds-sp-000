{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will be analyzing the Northwind database to generate hypothesis and test them using statistical and stocastic methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from statsmodels.stats.multicomp import MultiComparison \n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from khantools import *\n",
    "\n",
    "conn = sqlite3.connect('Northwind_small.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "q7= \"\"\"\n",
    "    SELECT \n",
    "        \n",
    "        P.ProductName,\n",
    "        OD.Quantity,\n",
    "        OD.UnitPrice,\n",
    "        OD.Discount AS DiscRate,\n",
    "        OD.Id as ODRecords,\n",
    "        (OD.UnitPrice * OD.Quantity) * (OD.Discount) AS DiscAmount,\n",
    "        (OD.UnitPrice * OD.Quantity) * (1 - OD.Discount) AS TotalPaid,\n",
    "        E.Firstname || ' ' || E.LastName AS EmpName,\n",
    "        E.Region AS EmpRegion,\n",
    "        C.Region AS CustRegion,\n",
    "        S.Region AS SuppRegion\n",
    "        \n",
    "        \n",
    "    FROM OrderDetail as OD\n",
    "    JOIN `Order` AS O ON OD.OrderID = O.Id \n",
    "    JOIN Product as P ON OD.ProductId = P.Id\n",
    "    JOIN Employee AS E on E.Id = O.EmployeeId\n",
    "    JOIN Customer AS C on C.Id = O.CustomerId\n",
    "    JOIN Supplier AS S on P.SupplierId = S.Id\n",
    "  \n",
    "    \n",
    "    \n",
    "    ORDER BY EmployeeId ASC\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q7)\n",
    "names = [description[0] for description in cursor.description]\n",
    "df_master = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "display(df_master)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Northwind_ERD_updated.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Testing Functions \n",
    "\n",
    "Lets write out some functions to make it easier to test our hypotheses down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cohen_d(control, experiment):\n",
    "   \n",
    "    diff = experiment.mean() - control.mean()\n",
    "\n",
    "    n1, n2 = len(experiment), len(control)\n",
    "    var1 = experiment.var()\n",
    "    var2 = control.var()\n",
    "\n",
    "    # Calculate the pooled threshold as shown earlier\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    \n",
    "    # Calculate Cohen's d statistic\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(a,b,size):\n",
    "    \"\"\"  Runs a monte carlo simulations on two pandas series (a and b)  \"\"\"\n",
    "    \n",
    "\n",
    "    diff_mu_a_b = np.mean(b) - np.mean(a)\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    union = a.append(b,ignore_index=True)\n",
    "       \n",
    "    \n",
    "    for i in range(size):\n",
    "       \n",
    "        \n",
    "        ai = union.sample(len(a))\n",
    "        \n",
    "        \n",
    "        bi = union.drop(ai.index)\n",
    "                 \n",
    "        diff_mu_ai_bi = np.mean(bi) - np.mean(ai)\n",
    "        \n",
    "        if diff_mu_ai_bi >= diff_mu_a_b:\n",
    "            num +=1\n",
    "            \n",
    "        denom += 1\n",
    "        \n",
    "    p_val = num/denom\n",
    "    print(\"In {} samples, The difference in mean between the experimental group and control group was higher than the observed values {} % percent of the time\".format(size,p_val*100))\n",
    "    return p_val\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hyp_test_mean(control,experiment):\n",
    "    \n",
    "    \"\"\" Tests the null hypothesis that an experimental sample comes from the same population as a control sample\n",
    "        Runs a students t-test, a Welch's t-test and a Mann Whitney test, and then indicated which results are most reliable\n",
    "        based on whether the assumptions for each respective test have been met or not. \n",
    "\n",
    "        Samples must be passed in as pandas series. \n",
    "    \"\"\"\n",
    "    \n",
    "# 1. Test variances\n",
    "    w,p_same_var = stats.levene(control,experiment)\n",
    "# 2. Test nromality\n",
    "    w,p_norm_a = stats.normaltest(control)\n",
    "    w,p_norm_b = stats.normaltest(experiment)\n",
    "    \n",
    "# 3. Run tests\n",
    "    \n",
    "    t_test_result = stats.ttest_ind(control,experiment)[1]\n",
    "    welch_result = stats.ttest_ind(control,experiment,equal_var=False)[1]\n",
    "    mann_whitney_u = stats.mannwhitneyu(control,experiment)[1]\n",
    "    \n",
    "# 4. Choose best test\n",
    "\n",
    "    norm_pass = ((p_norm_a >= 0.05) and (p_norm_b >= 0.05)) or ( (len(control) > 30) and (len(experiment) > 30) )\n",
    "    var_pass = p_same_var >= 0.05\n",
    "    \n",
    "    if var_pass and norm_pass:\n",
    "        t_test_notes = \"1 !!Best Test!!\"\n",
    "        welch_notes = \"not used; t-test assumptions met\"\n",
    "        mann_whitney_u_notes = \"not needed; t-test assumptions met\"\n",
    "    elif norm_pass and not var_pass:\n",
    "        welch_notes = \"1 !!Best Test!!\"\n",
    "        t_test_notes = \"not used: assumptions not met\"\n",
    "        mann_whitney_u_notes = \"not needed: Welch's assumptions met\"\n",
    "    else:\n",
    "        welch_notes = \"not used: assumptions not met\"\n",
    "        t_test_notes = \"not used: assumptions not met\"\n",
    "        mann_whitney_u_notes = \"1 !!Best Test!!\"\n",
    "\n",
    "# 5. results in df\n",
    "    test_name = ['t_test','Welch\\'s t-test','Mann Whitney U']\n",
    "    df_dict={\n",
    "        'Difference in means': [0,(np.mean(experiment) - np.mean(control))],\n",
    "        'Cohen\\'s d': [0,Cohen_d(control,experiment)],\n",
    "        'Sample A normality':[p_norm_a,'0'],\n",
    "        'Samp B normality':[p_norm_b,'0'],\n",
    "        'Variance similarity': [p_same_var,'0'],\n",
    "        't_test':[t_test_result,t_test_notes],\n",
    "        'Welch\\'s t-test' :[welch_result,welch_notes],\n",
    "        'Mann Whitney U':[mann_whitney_u,mann_whitney_u_notes]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(df_dict,orient = 'index',columns=['p_value','notes'])\n",
    "    df['Null Rejected'] = (df['p_value'] < 0.05)\n",
    "    df['p_value'].round(4)\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Hypothesis: Does discount amount have a statistically significant effect on the quantity of a product in an order? If so, at what level(s) of discount?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Null Hypthesis:</b>\n",
    "Discounts do not cause people to purchase greater quantities of products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Alternative Hypothesis:</b>People order a product in greater quantities if they are offered a discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the relevant data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ______________Dataframe with Quantity and Discount data on all orders_______________\n",
    "\n",
    "q1= \"\"\"\n",
    "    SELECT Quantity,Discount \n",
    "    FROM OrderDetail \n",
    "    GROUP BY `Id`\n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q1)\n",
    "names = [description[0] for description in cursor.description]\n",
    "\n",
    "df_all_orders = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "\n",
    "\n",
    "# ______________Control Dataframe; Only has data just for orders where no discounts were applied______________\n",
    "\n",
    "q2= \"\"\"\n",
    "    SELECT Quantity,Discount\n",
    "    FROM OrderDetail \n",
    "    WHERE Discount = 0 \n",
    "    GROUP BY `Id` \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q2)\n",
    "names = [description[0] for description in cursor.description]\n",
    "\n",
    "df_no_discounts = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "\n",
    "\n",
    "# ______________Experimental Dataframe; Only has data just for orders where no discounts were applied______________\n",
    "\n",
    "\n",
    "q3= \"\"\"\n",
    "    SELECT Quantity,Discount\n",
    "    FROM OrderDetail \n",
    "    WHERE Discount != 0 \n",
    "    GROUP BY `Id` \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q3)\n",
    "names = [description[0] for description in cursor.description]\n",
    "\n",
    "df_with_discounts = pd.DataFrame(cursor.fetchall(),columns=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Null Hypothesis\n",
    "\n",
    "Let us test the null hypothesis that the average quantity ordered is the same for orders with discounts applied as it is for orders with no discounts applied using the hypothesis testing pipeline function we defined above. \n",
    "\n",
    "This pipeline will check the sample sizes, whether the two samples are normally distributed and whether they have equal variance. It will then run a student's t-test, a Welch's t test andr a Mann Whitney U test on both samples, and tell us in the notes which test is most reliable based on whether or not the normality and variance assumptions have been met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = hyp_test_mean(df_no_discounts['Quantity'],df_with_discounts['Quantity'])\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we the null hypothesis has been rejected by all tests, including the Welch's test which is indicated as being the best one given the status of normality and variance of the samples.\n",
    "\n",
    "Lets run a Monte Carlo simualtion to double check the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = monte_carlo(df_no_discounts['Quantity'],df_with_discounts['Quantity'],10000)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUr Monte Carlo simulation seems to confirm what our Welch's t-test. In 10,000 samples we did not find even one where the mean of a randomly sampled group of the same size as the experimental group was higher than the mean of the control group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is a statistically significant increase in quantity ordered if a discount is being applied to the item in the order. The size of this increase for all levels of discount can be seen by the Cohen's d of 0.286 which is a small to moderate effect on the quantity ordered. \n",
    "\n",
    "Lets move to the second part of the question and take a closer look at effect sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect sizes at different levels of discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start of by running a pairwise Tukey's test on the discount levels to quickly get a sense of what the effect size is at different levels of discounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mc = MultiComparison(df_all_orders['Quantity'],df_all_orders['Discount'])\n",
    "result = mc.tukeyhsd()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting: it seems there is a statistically significant difference in means at most of the discount levels. However, this resul oject is a little hard to interpret and has data on comparisons between discount levels, which is not pertinent to our inquiry. \n",
    "\n",
    "Lets manually examine the discount levels and their attendant effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_levels = df_with_discounts['Discount'].value_counts()\n",
    "display(discount_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there are some discount percentages that are rarely applied. They will most likely just add noise to our analysis and it might be best to dispense with them. Lets also convert the relevant discount sizes into a list we can iterate through later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_list = list(discount_levels[discount_levels > 10].index)\n",
    "display(discount_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate the average quantity and Cohen's d at each relevant discount level. \n",
    "The code below will return a data frame with the mean quantity ordered, the difference from the null mean and the Cohen's for each level of discount. We will then plot the Cohen's d to assess the effect of each discount level on the quantity ordered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "\n",
    "for i in discount_list:\n",
    "    disc_df = df_with_discounts[df_with_discounts['Discount'] == i]['Quantity']\n",
    "    cohen_d= Cohen_d(df_no_discounts['Quantity'],disc_df)\n",
    "    mean = disc_df.mean()\n",
    "    mean_diff = disc_df.mean() - df_no_discounts['Quantity'].mean()\n",
    "    df_dict[i] = [mean, mean_diff,cohen_d]\n",
    "\n",
    "effects_df = pd.DataFrame.from_dict(df_dict,orient='index',columns=['Mean','Mean Difference','Cohen\\'s D'])\n",
    "display(effects_df)   \n",
    "\n",
    "effects_df['Cohen\\'s D'].plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that discounts of 5,20, 15 and 25 percent have the most effect on quantity ordered. What is interesting is that a 10% discount seems to have less of an effect on the quantity ordered. Perhaps its worth exploring the data further to see when a 10 percent discount is applied and what about those situations might lead to a reduced effect on quantity ordered. However, based just off the data we have before us, it might make sense to phase out the 10% discount altogether or reduce it to 5%. By doing so, we will offer less of a discount and thus lose less revenue since a 10% discount is not having much of an effect on increasing sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Hypothesis: Does discount level lead to higher revenue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Null Hypothesis:</b> The revenue generated by discounted orders is the same as the revenue from ordes where not discount was applied. \n",
    "\n",
    "<b> Alt Hypothesis:</b> Discounted orders generate more revenue. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4= \"\"\"\n",
    "    SELECT       \n",
    "        Discount,          \n",
    "        ((OrderDetail.UnitPrice * Quantity) * (1 - Discount)) AS TotalPaid\n",
    "    FROM OrderDetail \n",
    "    JOIN Product ON OrderDetail.ProductId = Product.Id\n",
    "    WHERE Discount !=0\n",
    "    ORDER BY TotalPaid DESC\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q4)\n",
    "names = [description[0] for description in cursor.description]\n",
    "df_totals_disc = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "display(df_totals_disc)\n",
    "df_totals_disc['TotalPaid'].hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5= \"\"\"\n",
    "    SELECT       \n",
    "        Discount,          \n",
    "        ((OrderDetail.UnitPrice * Quantity) * (1 - Discount)) AS TotalPaid\n",
    "    FROM OrderDetail \n",
    "    JOIN Product ON OrderDetail.ProductId = Product.Id\n",
    "    WHERE Discount = 0\n",
    "    ORDER BY TotalPaid DESC\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q5)\n",
    "names = [description[0] for description in cursor.description]\n",
    "df_totals_no_disc = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "display(df_totals_no_disc)\n",
    "df_totals_no_disc['TotalPaid'].hist(bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x:'%.6f' % x)\n",
    "control2 = df_totals_no_disc['TotalPaid']\n",
    "experiment2 = df_totals_disc['TotalPaid']\n",
    "\n",
    "hyp2 = hyp_test_mean(control2,experiment2)\n",
    "display(hyp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_p = monte_carlo(control2,experiment2,30000)\n",
    "print(h2_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that a ttest was the most reliable mathematical test, given that the variances of the two samples was very similar (which we saw both from the Leven calculated p-value in the test report as well as by eyeballing the histograms). Note that this is a two sided pval: the probability of observing a mean at least as high as the experimental mean would be half of this i.e. around 0.15. This is further coroborated by our Monte Carlo simlation which also shows that 15% of random samples pulled from the pooled samples resulted in a mean difference higher than that observed in the original samples. This is essentially the same value as our p-value. from the t test. \n",
    "\n",
    "Based on the tests, we are unable to dismiss the null hypothesis that purchases with discounts applied yield the same revenue are orders without any discounts applied. Therefore, it seems that there is no statistically significant difference in the revenue generated by discounted orders versus orders without discounts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Hypothesis: Do some employees have better sales skills than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next issue I want to explore is the sales ability of employees. My question is, do some employees sell more than others? \n",
    "\n",
    "Exploring this question is a bit trickier than it appears. Specifically,we have to first select an appropriate metric that can be said to represent an employees sales ability. \n",
    "\n",
    "We cant use total value of all purchases they processed, since some employees have been working longer than others. I feel the best metric to use here is \"revenue per order\". This is a fair metric as it only asks how much money an employee makes on average per order. This accounts for their different lengths of employment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp_performance = df_master.groupby('EmpName').agg({'TotalPaid':'sum','DiscAmount':'sum','OrderID':'count'})\n",
    "\n",
    "df_emp_performance['RevPerOrder'] = df_emp_performance['TotalPaid'] /df_emp_performance['OrderID']\n",
    "df_emp_performance.sort_values(by='RevPerOrder',ascending=False,inplace=True)\n",
    "\n",
    "df_emp_performance.plot(y='RevPerOrder',kind='barh',figsize=(10,10))\n",
    "display(df_emp_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some employees seem to generate higher revenue per order than others. Robert King seems to be doing better than everyone!  To make this determination, we will compare the Total Paid for each order for Robert King against the total paid for all other employees, and see if the difference is significant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x:'%.6f' % x)\n",
    "\n",
    "highest_emp = df_master[df_master['EmpName'] == 'Robert King']['TotalPaid']\n",
    "remain_emps = df_master.drop(highest_emp.index)['TotalPaid']\n",
    "\n",
    "emp_hyp = hyp_test_mean(remain_emps,highest_emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. The Welch's t-test leads us to believe that we cannot prove our hypothesis that Robert King's higher sales performance is due to anything other than random chance. We see from our results table that a Welch's t-test is the preferred test. This is probably because the probability of the variances being equal is around 3.3%. \n",
    "\n",
    "However, it is interesting to note that under a regular t-test, our null hypothesis is rejected and Rober King's performance would seem to be significantly better than those of his peers. Given that a 3.3% chance of the variances being similar is not too far below our threshold of 5%, perhaps there may be some correlation after all. \n",
    "\n",
    "Lets run a Monte Carlo and see what hapens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_monte = monte_carlo(remain_emps,highest_emp,10000)\n",
    "print(bob_monte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very interesting! Our Monte Carlo simulation seems to yield results virtually identical to what we saw with a standard t-test. The odds of seeing an average Total Paid per purchase that is as much or higher than what we observed for Robert King is 2.6%. \n",
    "\n",
    "As such, at this stage I am inclined to conclude that at least Robert King's higher average revenue per order is statistically significant and not merely due to random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounding variables\n",
    "\n",
    "\n",
    "However, even if random chance cannot account for the the higher revenues per order, there could be other explanations. \n",
    "\n",
    "For example, what if some employees generate more revenue because they sell to regions where customers spend more? Ofcourse, that could itself be a strategic decision on the employees part: they purposefully focus on higher spending regions. It could also be the case that employees are assigned regions, in which case we can hardly credit employees for generating higher revenue when they just got lucky and were assigned to a higher paid territory. \n",
    "\n",
    "It seems then that we must look at rates at which different employees sell to different regions, and whether these rates are essentially similar across employees or if some employees can be said to sell more in some regions than others (and whether these higher sales in particular regions could have been simply the result of random chance). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empterr = df_master[['EmpName','CustRegion','ODRecords']] #.pivot(index='CustRegion',columns='EmpName')\n",
    "df_empterr = df_empterr.pivot_table(index='CustRegion',columns='EmpName',aggfunc='count').fillna(1)\n",
    "df_empterr.columns = df_empterr.columns.droplevel() \n",
    "\n",
    "display(df_empterr)\n",
    "df_empterr.plot(subplots=True,kind='bar',layout=(3,3),figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We see from the above charts that employees seem to sell in different regions at almost exactly the same rates. This increases our confidence in our conclusion that Robert King truly is the King of Sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Hypothesis: Do customers prefer buying from the same region ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im curious to see if customers from particular regions have a tendency to prefer local products. \n",
    "\n",
    "<b>Null hypotheses:</b> The proportion of local products purchased by customers in a given region is not significantly different than the rate of local purchases for all the other customers from all other regions. \n",
    "\n",
    "<b> Alternative Hypothesis</b>: The customers in the given region are significantly more or less likely to purchase local products \n",
    "\n",
    "The code below will add up the number of local purchases made by customers in each region and compare that number to their total purchases. Then we will calculate similar proportions for all other customers and regions and compare the two proportions to see what the liklihood is of seeing the proportion of local purchases observed in this region given the expected average of local purchases we derived from the overall population. \n",
    "\n",
    "The resuling data frame will tell us whether we can reject our null hypothesis, and if so, whether the proportion of local purchases is significantly higher or lower, i.e. whether customers from that region prefer buying local or imported products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region_pref = df_master[['CustRegion','SuppRegion']]\n",
    "df_region_pref[\"SameReg?\"] =  df_region_pref['CustRegion'] == df_region_pref['SuppRegion']\n",
    "\n",
    "regions_group = df_region_pref.groupby('CustRegion')\n",
    "all_regions = list(regions_group.groups.keys())\n",
    "\n",
    "\n",
    "final_df = {}\n",
    "\n",
    "for region in all_regions:\n",
    "\n",
    "    \n",
    "    df_region = regions_group.get_group(region)\n",
    "    region_same_p = df_region['SameReg?'].sum()\n",
    "    seen_p = region_same_p/len(df_region)\n",
    "    \n",
    "    \n",
    "    df_control = df_region_pref.drop(labels=list(df_region.index),axis=0)\n",
    "    exp_p =  (df_control['SameReg?'].sum())/len(df_control)\n",
    "\n",
    "    p = stats.binom_test(x=region_same_p,n=len(df_region),p=exp_p)\n",
    "    \n",
    "    result = \"No\"\n",
    "    res_type=''\n",
    "    if p < 0.05 and seen_p < exp_p:\n",
    "        result = \"Yes\"\n",
    "        res_type = \"Imports\"\n",
    "    elif p < 0.05 and seen_p > exp_p:\n",
    "        res_type = 'Local'\n",
    "        \n",
    "    \n",
    "    final_df[region] = [exp_p,seen_p,p,result,res_type]\n",
    "    \n",
    "columns=[\n",
    "    'Population proportion of local products',\n",
    "    'Regions proportion of local purchases',\n",
    "    'P-val of binomial test',\n",
    "    'Null Rejected',\n",
    "    'Exports or Imports?'\n",
    "]\n",
    "    \n",
    "results = pd.DataFrame.from_dict(final_df,orient='index',columns=columns)\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "    \n",
    "    We can see from our results that the British Isles, Central America and South America seem to prefer imported goods, while North America and Western Europe prefer local goods. This is kind of in line with what we would expect knowing that North America and Europe are generaly places from where luxury food items originate. Now we have statistical evidence on which markets prefer imported vs local products and can modify sales strategy accordingly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "446px",
    "width": "301px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "614px",
    "left": "518px",
    "right": "20px",
    "top": "134px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
