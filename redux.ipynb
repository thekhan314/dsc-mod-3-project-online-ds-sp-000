{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Monte Carlo\n",
    "2. t-test\n",
    "2. Tukey\n",
    "3. Pipeline\n",
    "4. Hypothesis\n",
    "    1. DO bigger customers get a higher discount?\n",
    "    2. Do discounts lead to more revenue?\n",
    "   Beneficiaries of discounts?\n",
    "   \n",
    "OUTLIER REMOVAL!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductName</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>DiscRate</th>\n",
       "      <th>DiscAmount</th>\n",
       "      <th>TotalPaid</th>\n",
       "      <th>EmpName</th>\n",
       "      <th>CompanyName</th>\n",
       "      <th>ContactName</th>\n",
       "      <th>CustID</th>\n",
       "      <th>EmpRegion</th>\n",
       "      <th>CustRegion</th>\n",
       "      <th>OrderID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chang</td>\n",
       "      <td>50</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>152.00</td>\n",
       "      <td>608.00</td>\n",
       "      <td>Nancy Davolio</td>\n",
       "      <td>Ernst Handel</td>\n",
       "      <td>Roland Mendel</td>\n",
       "      <td>ERNSH</td>\n",
       "      <td>North America</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>10258/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chef Anton's Gumbo Mix</td>\n",
       "      <td>65</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>221.00</td>\n",
       "      <td>884.00</td>\n",
       "      <td>Nancy Davolio</td>\n",
       "      <td>Ernst Handel</td>\n",
       "      <td>Roland Mendel</td>\n",
       "      <td>ERNSH</td>\n",
       "      <td>North America</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>10258/5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mascarpone Fabioli</td>\n",
       "      <td>6</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30.72</td>\n",
       "      <td>122.88</td>\n",
       "      <td>Nancy Davolio</td>\n",
       "      <td>Ernst Handel</td>\n",
       "      <td>Roland Mendel</td>\n",
       "      <td>ERNSH</td>\n",
       "      <td>North America</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>10258/32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inlagd Sill</td>\n",
       "      <td>30</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>456.00</td>\n",
       "      <td>Nancy Davolio</td>\n",
       "      <td>Wartian Herkku</td>\n",
       "      <td>Pirkko Koskitalo</td>\n",
       "      <td>WARTH</td>\n",
       "      <td>North America</td>\n",
       "      <td>Scandinavia</td>\n",
       "      <td>10270/36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ipoh Coffee</td>\n",
       "      <td>25</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>920.00</td>\n",
       "      <td>Nancy Davolio</td>\n",
       "      <td>Wartian Herkku</td>\n",
       "      <td>Pirkko Koskitalo</td>\n",
       "      <td>WARTH</td>\n",
       "      <td>North America</td>\n",
       "      <td>Scandinavia</td>\n",
       "      <td>10270/43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>Teatime Chocolate Biscuits</td>\n",
       "      <td>35</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>322.00</td>\n",
       "      <td>Anne Dodsworth</td>\n",
       "      <td>Hanari Carnes</td>\n",
       "      <td>Mario Pontes</td>\n",
       "      <td>HANAR</td>\n",
       "      <td>British Isles</td>\n",
       "      <td>South America</td>\n",
       "      <td>11022/19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>Gudbrandsdalsost</td>\n",
       "      <td>30</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1080.00</td>\n",
       "      <td>Anne Dodsworth</td>\n",
       "      <td>Hanari Carnes</td>\n",
       "      <td>Mario Pontes</td>\n",
       "      <td>HANAR</td>\n",
       "      <td>British Isles</td>\n",
       "      <td>South America</td>\n",
       "      <td>11022/69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>Sir Rodney's Scones</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>Anne Dodsworth</td>\n",
       "      <td>Blauer See Delikatessen</td>\n",
       "      <td>Hanna Moos</td>\n",
       "      <td>BLAUS</td>\n",
       "      <td>British Isles</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>11058/21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>Camembert Pierrot</td>\n",
       "      <td>21</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>714.00</td>\n",
       "      <td>Anne Dodsworth</td>\n",
       "      <td>Blauer See Delikatessen</td>\n",
       "      <td>Hanna Moos</td>\n",
       "      <td>BLAUS</td>\n",
       "      <td>British Isles</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>11058/60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>Sirop d'érable</td>\n",
       "      <td>4</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>Anne Dodsworth</td>\n",
       "      <td>Blauer See Delikatessen</td>\n",
       "      <td>Hanna Moos</td>\n",
       "      <td>BLAUS</td>\n",
       "      <td>British Isles</td>\n",
       "      <td>Western Europe</td>\n",
       "      <td>11058/61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2078 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ProductName  Quantity  UnitPrice  DiscRate  DiscAmount  \\\n",
       "0                          Chang        50       15.2       0.2      152.00   \n",
       "1         Chef Anton's Gumbo Mix        65       17.0       0.2      221.00   \n",
       "2             Mascarpone Fabioli         6       25.6       0.2       30.72   \n",
       "3                    Inlagd Sill        30       15.2       0.0        0.00   \n",
       "4                    Ipoh Coffee        25       36.8       0.0        0.00   \n",
       "...                          ...       ...        ...       ...         ...   \n",
       "2073  Teatime Chocolate Biscuits        35        9.2       0.0        0.00   \n",
       "2074            Gudbrandsdalsost        30       36.0       0.0        0.00   \n",
       "2075         Sir Rodney's Scones         3       10.0       0.0        0.00   \n",
       "2076           Camembert Pierrot        21       34.0       0.0        0.00   \n",
       "2077              Sirop d'érable         4       28.5       0.0        0.00   \n",
       "\n",
       "      TotalPaid         EmpName              CompanyName       ContactName  \\\n",
       "0        608.00   Nancy Davolio             Ernst Handel     Roland Mendel   \n",
       "1        884.00   Nancy Davolio             Ernst Handel     Roland Mendel   \n",
       "2        122.88   Nancy Davolio             Ernst Handel     Roland Mendel   \n",
       "3        456.00   Nancy Davolio           Wartian Herkku  Pirkko Koskitalo   \n",
       "4        920.00   Nancy Davolio           Wartian Herkku  Pirkko Koskitalo   \n",
       "...         ...             ...                      ...               ...   \n",
       "2073     322.00  Anne Dodsworth            Hanari Carnes      Mario Pontes   \n",
       "2074    1080.00  Anne Dodsworth            Hanari Carnes      Mario Pontes   \n",
       "2075      30.00  Anne Dodsworth  Blauer See Delikatessen        Hanna Moos   \n",
       "2076     714.00  Anne Dodsworth  Blauer See Delikatessen        Hanna Moos   \n",
       "2077     114.00  Anne Dodsworth  Blauer See Delikatessen        Hanna Moos   \n",
       "\n",
       "     CustID      EmpRegion      CustRegion   OrderID  \n",
       "0     ERNSH  North America  Western Europe   10258/2  \n",
       "1     ERNSH  North America  Western Europe   10258/5  \n",
       "2     ERNSH  North America  Western Europe  10258/32  \n",
       "3     WARTH  North America     Scandinavia  10270/36  \n",
       "4     WARTH  North America     Scandinavia  10270/43  \n",
       "...     ...            ...             ...       ...  \n",
       "2073  HANAR  British Isles   South America  11022/19  \n",
       "2074  HANAR  British Isles   South America  11022/69  \n",
       "2075  BLAUS  British Isles  Western Europe  11058/21  \n",
       "2076  BLAUS  British Isles  Western Europe  11058/60  \n",
       "2077  BLAUS  British Isles  Western Europe  11058/61  \n",
       "\n",
       "[2078 rows x 13 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from statsmodels.stats.multicomp import MultiComparison \n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from khantools import *\n",
    "\n",
    "conn = sqlite3.connect('Northwind_small.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "q7= \"\"\"\n",
    "    SELECT \n",
    "        \n",
    "        P.ProductName,\n",
    "        OD.Quantity,\n",
    "        OD.UnitPrice,\n",
    "        OD.Discount AS DiscRate,\n",
    "        (OD.UnitPrice * OD.Quantity) * (OD.Discount) AS DiscAmount,\n",
    "        (OD.UnitPrice * OD.Quantity) * (1 - OD.Discount) AS TotalPaid,\n",
    "        E.Firstname || ' ' || E.LastName AS EmpName,\n",
    "        C.CompanyName,\n",
    "        C.ContactName,\n",
    "        C.Id AS CustID,\n",
    "        E.Region AS EmpRegion,\n",
    "        C.Region AS CustRegion,\n",
    "        OD.Id As OrderID\n",
    "        \n",
    "    FROM OrderDetail as OD\n",
    "    JOIN `Order` AS O ON OD.OrderID = O.Id \n",
    "    JOIN Product as P ON OD.ProductId = P.Id\n",
    "    JOIN Employee AS E on E.Id = O.EmployeeId\n",
    "    JOIN Customer AS C on C.Id = O.CustomerId\n",
    "  \n",
    "    \n",
    "    \n",
    "    ORDER BY EmployeeId ASC\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q7)\n",
    "names = [description[0] for description in cursor.description]\n",
    "df_raw = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "df_master = rm_outliers_threshold(df_raw,['Quantity','DiscRate', 'TotalPaid'])\n",
    "display(df_master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Northwind_ERD_updated.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Testing Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cohen_d(control, experiment):\n",
    "   \n",
    "    diff = experiment.mean() - control.mean()\n",
    "\n",
    "    n1, n2 = len(experiment), len(control)\n",
    "    var1 = experiment.var()\n",
    "    var2 = control.var()\n",
    "\n",
    "    # Calculate the pooled threshold as shown earlier\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    \n",
    "    # Calculate Cohen's d statistic\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(a,b,size):\n",
    "    \"\"\"  Runs a monte carlo simulations on two pandas series (a and b)  \"\"\"\n",
    "    \n",
    "\n",
    "    diff_mu_a_b = np.mean(b) - np.mean(a)\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    union = a.append(b,ignore_index=True)\n",
    "       \n",
    "    \n",
    "    for i in range(size):\n",
    "       \n",
    "        \n",
    "        ai = union.sample(len(a))\n",
    "        \n",
    "        \n",
    "        bi = union.drop(ai.index)\n",
    "                 \n",
    "        diff_mu_ai_bi = np.mean(bi) - np.mean(ai)\n",
    "        \n",
    "        if diff_mu_ai_bi >= diff_mu_a_b:\n",
    "            num +=1\n",
    "            \n",
    "        denom += 1\n",
    "        \n",
    "    p_val = num/denom\n",
    "    print(\"In {} samples, The mean of the experimental group was higher than the control group {} % percent of the time\".format(size,p_val*100))\n",
    "    return p_val\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hyp_test_mean(control,experiment):\n",
    "    \n",
    "    \"\"\" Tests the null hypothesis that an experimental sample comes from the same population as a control sample\n",
    "        Runs a students t-test, a Welch's t-test and a Mann Whitney test, and then indicated which results are most reliable\n",
    "        based on whether the assumptions for each respective test have been met or not. \n",
    "\n",
    "        Samples must be passed in as pandas series. \n",
    "    \"\"\"\n",
    "    \n",
    "# 1. Test variances\n",
    "    w,p_same_var = stats.levene(control,experiment)\n",
    "# 2. Test nromality\n",
    "    w,p_norm_a = stats.normaltest(control)\n",
    "    w,p_norm_b = stats.normaltest(experiment)\n",
    "    \n",
    "# 3. Run tests\n",
    "    \n",
    "    t_test_result = stats.ttest_ind(control,experiment)[1]\n",
    "    welch_result = stats.ttest_ind(control,experiment,equal_var=False)[1]\n",
    "    mann_whitney_u = stats.mannwhitneyu(control,experiment)[1]\n",
    "    \n",
    "# 4. Choose best test\n",
    "\n",
    "    norm_pass = ((p_norm_a >= 0.05) and (p_norm_b >= 0.05)) or ( (len(control) > 30) and (len(experiment) > 30) )\n",
    "    var_pass = p_same_var >= 0.05\n",
    "    \n",
    "    if var_pass and norm_pass:\n",
    "        t_test_notes = \"1 !!Best Test!!\"\n",
    "        welch_notes = \"not used; t-test assumptions met\"\n",
    "        mann_whitney_u_notes = \"not needed; t-test assumptions met\"\n",
    "    elif norm_pass and not var_pass:\n",
    "        welch_notes = \"1 !!Best Test!!\"\n",
    "        t_test_notes = \"not used: assumptions not met\"\n",
    "        mann_whitney_u_notes = \"not needed: Welch's assumptions met\"\n",
    "    else:\n",
    "        welch_notes = \"not used: assumptions not met\"\n",
    "        t_test_notes = \"not used: assumptions not met\"\n",
    "        mann_whitney_u_notes = \"1 !!Best Test!!\"\n",
    "\n",
    "# 5. results in df\n",
    "    test_name = ['t_test','Welch\\'s t-test','Mann Whitney U']\n",
    "    df_dict={\n",
    "        'Difference in means': [0,(np.mean(experiment) - np.mean(control))],\n",
    "        'Cohen\\'s d': [0,Cohen_d(control,experiment)],\n",
    "        'Sample A normality':[p_norm_a,'0'],\n",
    "        'Samp B normality':[p_norm_b,'0'],\n",
    "        'Variance similarity': [p_same_var,'0'],\n",
    "        't_test':[t_test_result,t_test_notes],\n",
    "        'Welch\\'s t-test' :[welch_result,welch_notes],\n",
    "        'Mann Whitney U':[mann_whitney_u,mann_whitney_u_notes]\n",
    "        \n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(df_dict,orient = 'index',columns=['p_value','notes'])\n",
    "    df['Null Rejected'] = (df['p_value'] < 0.05)\n",
    "    df['p_value'].round(4)\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Hypothesis: **_Does discount amount have a statistically significant effect on the quantity of a product in an order? If so, at what level(s) of discount?_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Null Hypthesis:</b>\n",
    "Discounts do not cause people to purchase greater quantities of products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Alternative Hypothesis:</b>People order a product in greater quantities if they are offered a discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load the relevant data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ______________Dataframe with Quantity and Discount data on all orders_______________\n",
    "\n",
    "q1= \"\"\"\n",
    "    SELECT Quantity,Discount \n",
    "    FROM OrderDetail \n",
    "    GROUP BY `Id`\n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q1)\n",
    "names = [description[0] for description in cursor.description]\n",
    "\n",
    "df_all_orders = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "\n",
    "\n",
    "# ______________Control Dataframe; Only has data just for orders where no discounts were applied______________\n",
    "\n",
    "q2= \"\"\"\n",
    "    SELECT Quantity,Discount\n",
    "    FROM OrderDetail \n",
    "    WHERE Discount = 0 \n",
    "    GROUP BY `Id` \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q2)\n",
    "names = [description[0] for description in cursor.description]\n",
    "\n",
    "df_no_discounts = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "\n",
    "\n",
    "# ______________Experimental Dataframe; Only has data just for orders where no discounts were applied______________\n",
    "\n",
    "\n",
    "q3= \"\"\"\n",
    "    SELECT Quantity,Discount\n",
    "    FROM OrderDetail \n",
    "    WHERE Discount != 0 \n",
    "    GROUP BY `Id` \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q3)\n",
    "names = [description[0] for description in cursor.description]\n",
    "\n",
    "df_with_discounts = pd.DataFrame(cursor.fetchall(),columns=names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Null Hypothesis\n",
    "\n",
    "Let us test the null hypothesis that the average quantity ordered is the same for orders with discounts applied as it is for orders with no discounts applied using the hypothesis testing pipeline function we defined above. \n",
    "\n",
    "This pipeline where check the sample sizes, whether the two samples are normally distributed and whether they have equal variance. It will then run a student's t-test, a Welch's t test andr a Mann Whitney U test on both samples, and tell us in the notes which test is most reliable based on whether or not the normality and variance assumptions have been met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = hyp_test_mean(df_no_discounts['Quantity'],df_with_discounts['Quantity'])\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we the null hypothesis has been rejected by all tests, including the Welch's test which is indicated as being the best one given the status of normality and variance of the samples.\n",
    "\n",
    "Lets run a Monte Carlo simualtion to double check the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = monte_carlo(df_no_discounts['Quantity'],df_with_discounts['Quantity'],10000)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there is a statistically significant increase in quantity ordered if a discount is being applied to the item in the order. The size of this increase for all levels of discount can be seen by the Cohen's d of 0.286 which is a small to moderate effect on the quantity ordered. \n",
    "\n",
    "Lets move to the second part of the question and take a closer look at effect sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect sizes at different levels of discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start of by running a pairwise Tukey's test on the discount levels to quickly get a sense of what the effect size is at different levels of discounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mc = MultiComparison(df_all_orders['Quantity'],df_all_orders['Discount'])\n",
    "result = mc.tukeyhsd()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting: it seems there is a statistically significant difference in means at most of the discount levels. However, this resul oject is a little hard to interpret and has data on comparisons between discount levels, which is not pertinent to our inquiry. \n",
    "\n",
    "Lets manually examine the discount levels and their attendant effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_levels = df_with_discounts['Discount'].value_counts()\n",
    "display(discount_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there are some discount percentages that are rarely applied. They will most likely just add noise to our analysis and it might be best to dispense with them. Lets also convert the relevant discount sizes into a list we can iterate through later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_list = list(discount_levels[discount_levels > 10].index)\n",
    "display(discount_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate the average quantity and Cohen's d at each relevant discount level. \n",
    "The code below will return a data frame with the mean quantity ordered, the difference from the null mean and the Cohen's for each level of discount. We will then plot the Cohen's d to assess the effect of each discount level on the quantity ordered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "\n",
    "for i in discount_list:\n",
    "    disc_df = df_with_discounts[df_with_discounts['Discount'] == i]['Quantity']\n",
    "    cohen_d= Cohen_d(df_no_discounts['Quantity'],disc_df)\n",
    "    mean = disc_df.mean()\n",
    "    mean_diff = disc_df.mean() - df_no_discounts['Quantity'].mean()\n",
    "    df_dict[i] = [mean, mean_diff,cohen_d]\n",
    "\n",
    "effects_df = pd.DataFrame.from_dict(df_dict,orient='index',columns=['Mean','Mean Difference','Cohen\\'s D'])\n",
    "display(effects_df)   \n",
    "\n",
    "effects_df['Cohen\\'s D'].plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that discounts of 5,20, 15 and 25 percent have the most effect on quantity ordered. What is interesting is that a 10% discount seems to have less of an effect on the quantity ordered. Perhaps its worth exploring the data further to see when a 10 percent discount is applied and what about those situations might lead to a reduced effect on quantity ordered. However, based just off the data we have before us, it might make sense to phase out the 10% discount altogether or reduce it to 5%. By doing so, we will offer less of a discount and thus lose less revenue since a 10% discount is not having much of an effect on increasing sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Hypothesis: does discount level lead to higher revenue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Null Hypothesis:</b> The revenue generated by discounted orders is the same as the revenue from ordes where not discount was applied. \n",
    "\n",
    "<b> Alt Hypothesis:</b> Discounted orders generate more revenue. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4= \"\"\"\n",
    "    SELECT       \n",
    "        Discount,          \n",
    "        ((OrderDetail.UnitPrice * Quantity) * (1 - Discount)) AS TotalPaid\n",
    "    FROM OrderDetail \n",
    "    JOIN Product ON OrderDetail.ProductId = Product.Id\n",
    "    WHERE Discount !=0\n",
    "    ORDER BY TotalPaid DESC\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q4)\n",
    "names = [description[0] for description in cursor.description]\n",
    "df_totals_disc = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "display(df_totals_disc)\n",
    "df_totals_disc['TotalPaid'].hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5= \"\"\"\n",
    "    SELECT       \n",
    "        Discount,          \n",
    "        ((OrderDetail.UnitPrice * Quantity) * (1 - Discount)) AS TotalPaid\n",
    "    FROM OrderDetail \n",
    "    JOIN Product ON OrderDetail.ProductId = Product.Id\n",
    "    WHERE Discount = 0\n",
    "    ORDER BY TotalPaid DESC\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q5)\n",
    "names = [description[0] for description in cursor.description]\n",
    "df_totals_no_disc = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "display(df_totals_no_disc)\n",
    "df_totals_no_disc['TotalPaid'].hist(bins = 30)e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x:'%.6f' % x)\n",
    "control2 = df_totals_no_disc['TotalPaid']\n",
    "experiment2 = df_totals_disc['TotalPaid']\n",
    "\n",
    "hyp2 = hyp_test_mean(control2,experiment2)\n",
    "display(hyp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2_p = monte_carlo(control2,experiment2,30000)\n",
    "print(h2_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that a ttest was the most reliable mathematical test, given that the variances of the two samples was very similar (which we saw both from the Leven calculated p-value in the test report as well as by eyeballing the histograms). Note that this is a two sided pval: the probability of observing a mean at least as high as the experimental mean would be half of this i.e. around 0.15. This is further coroborated by our Monte Carlo simlation which also shows that 15% of random samples pulled from the pooled samples resulted in a mean difference higher than that observed in the original samples. This is essentially the same value as our p-value. from the t test. \n",
    "\n",
    "Based on the tests, we are unable to dismiss the null hypothesis that purchases with discounts applied yield the same revenue are orders without any discounts applied. Therefore, it seems that there is no statistically significant difference in the revenue generated by discounted orders versus orders without discounts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Hypothesis: Do some employees have better sales skills than others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next issue I want to explore is the sales ability of employees. My question is, do some employees sell more than others? \n",
    "\n",
    "Exploring this question is a bit trickier than it appears. Specifically,we have to first select an appropriate metric that can be said to represent an employees sales ability. \n",
    "\n",
    "We cant use total value of all purchases they processed, since some employees have been working longer than others. I feel the best metric to use here is \"revenue per order\". This is a fair metric as it only asks how much money an employee makes on average per order. This accounts for their different lengths of employment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp_performance = df_master.groupby('EmpName').agg({'TotalPaid':'sum','DiscAmount':'sum','OrderID':'count'})\n",
    "\n",
    "df_emp_performance['RevPerOrder'] = df_emp_performance['TotalPaid'] /df_emp_performance['OrderID']\n",
    "df_emp_performance.sort_values(by='RevPerOrder',ascending=False,inplace=True)\n",
    "\n",
    "df_emp_performance.plot(y='RevPerOrder',kind='barh',figsize=(10,10))\n",
    "display(df_emp_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some employees seem to generate higher revenue per order than others. But is this difference statistically significant or is simply due to random chance?\n",
    "\n",
    "To make this determination, we will compare the Total Paid for each order for each employee against the total paid for all other employees, and see if the difference is significant. \n",
    "\n",
    "Let's first do this for the employee with the highest revenue per order, Robert King. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x:'%.6f' % x)\n",
    "\n",
    "highest_emp = df_master[df_master['EmpName'] == 'Robert King']['TotalPaid']\n",
    "remain_emps = df_master.drop(highest_emp.index)['TotalPaid']\n",
    "\n",
    "emp_hyp = hyp_test_mean(remain_emps,highest_emp)\n",
    "bob_monte = monte_carlo(remain_emps,highest_emp,10000)\n",
    "print(bob_monte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth Hypothesis: Are employee sales distributed evenly across regions, or do some employees sell more in some regions than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7= \"\"\"\n",
    "    SELECT \n",
    "        \n",
    "        P.ProductName,\n",
    "        OD.Quantity,\n",
    "        OD.UnitPrice,\n",
    "        OD.Id as ODRecords,\n",
    "        OD.Discount AS DiscRate,\n",
    "        (OD.UnitPrice * OD.Quantity) * (OD.Discount) AS DiscAmount,\n",
    "        (OD.UnitPrice * OD.Quantity) * (1 - OD.Discount) AS TotalPaid,\n",
    "        E.Firstname || ' ' || E.LastName AS EmpName,\n",
    "        C.CompanyName,\n",
    "        C.Region AS CustRegion\n",
    "\n",
    "        \n",
    "    FROM OrderDetail as OD\n",
    "    JOIN `Order` AS O ON OD.OrderID = O.Id \n",
    "    JOIN Product as P ON OD.ProductId = P.Id\n",
    "    JOIN Employee AS E on E.Id = O.EmployeeId\n",
    "    JOIN Customer AS C on C.Id = O.CustomerId\n",
    "  \n",
    "    \n",
    "    \n",
    "    ORDER BY CustRegion\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "cursor = cur.execute(q7)\n",
    "names = [description[0] for description in cursor.description]\n",
    "df_master = pd.DataFrame(cursor.fetchall(),columns=names)\n",
    "\n",
    "\n",
    "display(df_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = df_master.groupby(by='CustRegion')['TotalPaid'].sum().sort_values(ascending=False).plot(kind='bar')\n",
    "display(df_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empterr = df_master[['EmpName','CustRegion','ODRecords']] #.pivot(index='CustRegion',columns='EmpName')\n",
    "df_empterr = df_empterr.pivot_table(index='CustRegion',columns='EmpName',aggfunc='count').fillna(0)\n",
    "display(df_empterr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "stat, p, dof, expected = chi2_contingency(df_empterr)\n",
    "print(p,dof,expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "emps_group = df_master.groupby('EmpName')\n",
    "emps = list(emps_group.groups.keys())\n",
    "regions = set(df_master['CustRegion'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "emps_group = df_master.groupby('EmpName')\n",
    "emps = list(emps_group.groups.keys())\n",
    "regions = set(df_master['CustRegion'].value_counts().index)\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(nrows=int(len(emps)/3),ncols=3,figsize=(20,20))\n",
    "\n",
    "for axnum,i in enumerate(emps):\n",
    "    emp_sales = emps_group.get_group(i).groupby('CustRegion')['TotalPaid'].sum()\n",
    "    \n",
    "    emp_regions = set(emp_sales.index)\n",
    "    missing_regions = list(regions-emp_regions)\n",
    "    \n",
    "    if len(missing_regions) != 0:\n",
    "        new_series = pd.Series([0 for x in missing_regions],index=missing_regions)\n",
    "        emp_sales = emp_sales.append(new_series).sort_index()\n",
    "    \n",
    "    #exp_df = df_master[df_master['EmpName'] != i].groupby(by='CustRegion')['TotalPaid'].sum().sort_index()\n",
    "    \n",
    "    emp_sales.plot(ax=ax.reshape(-1)[axnum],kind='bar',title=i,ylim=(0,110000),rot=30)\n",
    "    ax.reshape(-1)[axnum].set_ylabel('Total Sales')\n",
    "    ax.reshape(-1)[axnum].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'))\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "446px",
    "width": "301px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "618px",
    "left": "1155px",
    "right": "20px",
    "top": "223px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
